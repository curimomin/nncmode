"""
ë„¤ì´ë²„ ë‰´ìŠ¤ í†µí•© ìŠ¤í¬ë˜í¼ ë©”ì¸ ëª¨ë“ˆ
ê¸°ì‚¬ ì •ë³´ + ëŒ“ê¸€ ë°ì´í„°ë¥¼ articles.csvì™€ comments.csvë¡œ ì €ì¥
"""

import argparse
import logging
import time
import sys
import csv
import re
from datetime import datetime
from typing import List, Dict, Any, Optional
from urllib.parse import urlparse
from pathlib import Path

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from webdriver_manager.chrome import ChromeDriverManager
from tqdm import tqdm
import psutil

from utils import (
    load_config, load_urls, setup_logger, create_output_directory,
    validate_config, get_system_info
)

from drive_uploader import DriveUploader


class NaverNewsMainScraper:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ í†µí•© í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤"""

    def __init__(self, config: Dict[str, Any]):
        """
        í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”

        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config
        self.logger = logging.getLogger('naver_scraper')
        self.articles_data = []
        self.comments_data = []
        self.failed_urls = []
        self.selectors = config['naver_selectors']
        self.ui_labels = config['ui_labels']

        # ID ì¹´ìš´í„°
        self.article_id_counter = 1
        self.comment_id_counter = 1

    def _create_driver(self) -> webdriver.Chrome:
        """Chrome WebDriver ìƒì„±"""
        chrome_options = Options()

        # ì„¤ì •ì—ì„œ Chrome ì˜µì…˜ ì¶”ê°€
        for option in self.config.get('chrome_options', []):
            chrome_options.add_argument(option)

        # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì„¤ì •
        if self.config['scraping'].get('headless', True):
            chrome_options.add_argument('--headless')

        # ChromeDriver ì„œë¹„ìŠ¤ ìƒì„±
        service = Service(ChromeDriverManager().install())

        # WebDriver ìƒì„±
        driver = webdriver.Chrome(service=service, options=chrome_options)

        # Selenium ì„¤ì • ì ìš©
        selenium_config = self.config['selenium']
        driver.implicitly_wait(selenium_config.get('implicit_wait', 10))
        driver.set_page_load_timeout(
            selenium_config.get('page_load_timeout', 30))
        driver.set_script_timeout(selenium_config.get('script_timeout', 30))

        # ìœˆë„ìš° í¬ê¸° ì„¤ì •
        window_size = selenium_config.get('window_size', {})
        if window_size:
            driver.set_window_size(
                window_size.get('width', 1920),
                window_size.get('height', 1080)
            )

        return driver

    def _extract_text_by_selector(self, driver: webdriver.Chrome, selector: str) -> str:
        """
        CSS ì„ íƒìë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ

        Args:
            driver: WebDriver ì¸ìŠ¤í„´ìŠ¤
            selector: CSS ì„ íƒì (ì—¬ëŸ¬ ì„ íƒìë¥¼ ì½¤ë§ˆë¡œ êµ¬ë¶„)

        Returns:
            ì¶”ì¶œëœ í…ìŠ¤íŠ¸
        """
        selectors = [s.strip() for s in selector.split(',')]

        for sel in selectors:
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, sel)
                if elements:
                    # ì²« ë²ˆì§¸ ë§¤ì¹­ ìš”ì†Œì˜ í…ìŠ¤íŠ¸ ë°˜í™˜
                    text = elements[0].get_attribute(
                        'textContent') or elements[0].text
                    return text.strip()
            except Exception as e:
                self.logger.debug(f"ì„ íƒì '{sel}' ì‹¤íŒ¨: {e}")
                continue

        return ""

    def _extract_number_from_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ"""
        if not text:
            return ""

        numbers = re.findall(r'\d+', text)
        return numbers[0] if numbers else ""

    def _extract_article_data(self, driver: webdriver.Chrome, url: str) -> Optional[Dict[str, Any]]:
        """
        ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ

        Args:
            driver: WebDriver ì¸ìŠ¤í„´ìŠ¤
            url: ê¸°ì‚¬ URL

        Returns:
            ì¶”ì¶œëœ ê¸°ì‚¬ ë°ì´í„°
        """
        try:
            # í˜ì´ì§€ ë¡œë“œ
            driver.get(url)

            # í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°
            WebDriverWait(driver, self.config['selenium']['page_load_timeout']).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            scraped_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

            # ê¸°ë³¸ ë°ì´í„° ì¶”ì¶œ
            article_data = {
                'article_id': self.article_id_counter,
                'url': url,
                'title': self._extract_text_by_selector(driver, self.selectors['article']['title']),
                'content': self._extract_text_by_selector(driver, self.selectors['article']['content']),
                'author': self._extract_text_by_selector(driver, self.selectors['article']['author']),
                'publish_date': self._extract_text_by_selector(driver, self.selectors['article']['publish_date']),
                'category': self._extract_text_by_selector(driver, self.selectors['article']['category']),
                'like_count': self._extract_number_from_text(
                    self._extract_text_by_selector(
                        driver, self.selectors['article']['like_count'])
                ),
                'comment_count': self._extract_number_from_text(
                    self._extract_text_by_selector(
                        driver, self.selectors['article']['comment_count'])
                ),
                'scraped_at': scraped_at
            }

            # ëŒ“ê¸€ í†µê³„ í•„ë“œ ì´ˆê¸°í™” (ë¹ˆ ê°’)
            article_data.update({
                'active_comment_count': "",
                'deleted_comment_count': "",
                'removed_comment_count': "",
                'male_ratio': "",
                'female_ratio': "",
                'age_10s_ratio': "",
                'age_20s_ratio': "",
                'age_30s_ratio': "",
                'age_40s_ratio': "",
                'age_50s_ratio': "",
                'age_60plus_ratio': ""
            })

            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            if not article_data['title']:
                self.logger.warning(f"ì œëª©ì´ ì—†ëŠ” ê¸°ì‚¬: {url}")
                return None

            self.logger.debug(f"ê¸°ì‚¬ ë°ì´í„° ì¶”ì¶œ ì„±ê³µ: {url}")
            return article_data

        except Exception as e:
            self.logger.error(f"ê¸°ì‚¬ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
            return None

    def _navigate_to_comments_page(self, driver: webdriver.Chrome) -> bool:
        """
        ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ëŒ“ê¸€ í˜ì´ì§€ë¡œ ì´ë™

        Args:
            driver: WebDriver ì¸ìŠ¤í„´ìŠ¤

        Returns:
            ëŒ“ê¸€ í˜ì´ì§€ ì´ë™ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ëŒ“ê¸€ ë”ë³´ê¸° ë²„íŠ¼ ì°¾ê¸°
            comment_button = driver.find_element(
                By.CSS_SELECTOR,
                self.selectors['comment_navigation']['article_to_comment_button']
            )

            if comment_button.is_displayed():
                comment_button.click()

                # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
                time.sleep(3)
                return True
            else:
                self.logger.warning("ëŒ“ê¸€ ë²„íŠ¼ì´ ë³´ì´ì§€ ì•ŠìŒ")
                return False

        except NoSuchElementException:
            self.logger.warning("ëŒ“ê¸€ ë”ë³´ê¸° ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return False
        except Exception as e:
            self.logger.error(f"ëŒ“ê¸€ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: {e}")
            return False

    def _load_all_comments(self, driver: webdriver.Chrome) -> None:
        """
        ëŒ“ê¸€ í˜ì´ì§€ì—ì„œ ëª¨ë“  ëŒ“ê¸€ ë¡œë“œ (ë”ë³´ê¸° ë°˜ë³µ í´ë¦­)

        Args:
            driver: WebDriver ì¸ìŠ¤í„´ìŠ¤
        """
        try:
            while True:
                try:
                    # ëŒ“ê¸€ í˜ì´ì§€ì˜ ë”ë³´ê¸° ë²„íŠ¼ ì°¾ê¸°
                    more_button = driver.find_element(
                        By.CSS_SELECTOR,
                        self.selectors['comment_navigation']['comment_page_more_button']
                    )

                    if more_button.is_displayed():
                        more_button.click()
                        self.logger.debug("ëŒ“ê¸€ ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­")
                        time.sleep(3)  # ë¡œë”© ëŒ€ê¸°
                    else:
                        break

                except NoSuchElementException:
                    break  # ë”ë³´ê¸° ë²„íŠ¼ì´ ì—†ìœ¼ë©´ ëª¨ë“  ëŒ“ê¸€ ë¡œë“œ ì™„ë£Œ

        except Exception as e:
            self.logger.error(f"ëŒ“ê¸€ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

    def _extract_comment_stats(self, driver: webdriver.Chrome, article_data: Dict[str, Any]) -> None:
        """
        ëŒ“ê¸€ ì¼ë°˜í†µê³„ ì •ë³´ ì¶”ì¶œ ë° ê¸°ì‚¬ ë°ì´í„° ì—…ë°ì´íŠ¸

        Args:
            driver: WebDriver ì¸ìŠ¤í„´ìŠ¤
            article_data: ì—…ë°ì´íŠ¸í•  ê¸°ì‚¬ ë°ì´í„°
        """
        
        try:
            stat_items = driver.find_elements(
                By.CSS_SELECTOR, self.selectors['comment_stats']['stat_count_info'])

            # ê¸°ë³¸ê°’ ì„¤ì •
            article_data['active_comment_count'] = 0
            article_data['deleted_comment_count'] = 0
            article_data['removed_comment_count'] = 0

            for item in stat_items:
                title = self._extract_text_by_selector(
                    item, self.selectors['comment_stats']['stat_title'])
                value_text = self._extract_text_by_selector(
                    item, self.selectors['comment_stats']['stat_value'])
                value = self._extract_number_from_text(value_text)

                if self.ui_labels['comments']['current_comment_count'] in title:
                    article_data['active_comment_count'] = value
                elif self.ui_labels['comments']['deleted_comment_count'] in title:
                    article_data['deleted_comment_count'] = value
                elif self.ui_labels['comments']['removed_comment_count'] in title:
                    article_data['removed_comment_count'] = value

            self.logger.debug("  â”œâ”€ ëŒ“ê¸€ ì¼ë°˜í†µê³„ ì¶”ì¶œ ì™„ë£Œ")

        except Exception as e:
            self.logger.warning(f"ëŒ“ê¸€ ì¼ë°˜í†µê³„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    def _extract_comment_demographic_stats(self, driver: webdriver.Chrome, article_data: Dict[str, Any]) -> None:
        """
        ëŒ“ê¸€ ìƒì„¸í†µê³„ ì •ë³´ ì¶”ì¶œ ë° ê¸°ì‚¬ ë°ì´í„° ì—…ë°ì´íŠ¸

        Args:
            driver: WebDriver ì¸ìŠ¤í„´ìŠ¤
            article_data: ì—…ë°ì´íŠ¸í•  ê¸°ì‚¬ ë°ì´í„°
        """
        try:
            chart_wrap_element = driver.find_element(
                By.CSS_SELECTOR, self.selectors['comment_stats']['demographic_stats_container'])

            if not chart_wrap_element.is_displayed():
                self.logger.info("  â”œâ”€ ëŒ“ê¸€ ìƒì„¸í†µê³„ ì°¨íŠ¸ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return

            # ì„±ë³„ ë¹„ìœ¨ ì¶”ì¶œ
            article_data['male_ratio'] = self._extract_number_from_text(
                self._extract_text_by_selector(
                    driver, self.selectors['comment_stats']['male_ratio'])
            )
            article_data['female_ratio'] = self._extract_number_from_text(
                self._extract_text_by_selector(
                    driver, self.selectors['comment_stats']['female_ratio'])
            )

            # ì—°ë ¹ëŒ€ ë¹„ìœ¨ ì¶”ì¶œ
            age_chart_container = driver.find_element(
                By.CSS_SELECTOR, ".u_cbox_chart_age")
            age_items = age_chart_container.find_elements(
                By.CSS_SELECTOR, ".u_cbox_chart_progress")

            for item in age_items:
                age_text = self._extract_text_by_selector(
                    item, ".u_cbox_chart_cnt span")
                percentage_text = self._extract_text_by_selector(
                    item, ".u_cbox_chart_per")
                percentage = self._extract_number_from_text(percentage_text)

                if self.ui_labels['comments']['10s'] in age_text:
                    article_data['age_10s_ratio'] = percentage
                elif self.ui_labels['comments']['20s'] in age_text:
                    article_data['age_20s_ratio'] = percentage
                elif self.ui_labels['comments']['30s'] in age_text:
                    article_data['age_30s_ratio'] = percentage
                elif self.ui_labels['comments']['40s'] in age_text:
                    article_data['age_40s_ratio'] = percentage
                elif self.ui_labels['comments']['50s'] in age_text:
                    article_data['age_50s_ratio'] = percentage
                elif self.ui_labels['comments']['60s'] in age_text:
                    article_data['age_60plus_ratio'] = percentage

        except Exception as e:
            self.logger.warning(f"ëŒ“ê¸€ ìƒì„¸í†µê³„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    def _extract_comments_data(self, driver: webdriver.Chrome, article_id: int) -> None:
        """
        ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ

        Args:
            driver: WebDriver ì¸ìŠ¤í„´ìŠ¤
            article_id: ê¸°ì‚¬ ID
        """
        
        try:
            
            scraped_at = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

            # ëª¨ë“  ëŒ“ê¸€ ìš”ì†Œ ì°¾ê¸°
            comment_elements = driver.find_elements(
                By.CSS_SELECTOR,
                self.selectors['comments']['comment_list']
            )

            self.logger.info(f"  â”œâ”€ ë°œê²¬ëœ ëŒ“ê¸€ ìˆ˜: {len(comment_elements)}")
            
            comment_count = len(driver.find_elements(By.CSS_SELECTOR, self.selectors['comments']['comment_list']))

            for i in range(comment_count):

                try:
                  
                    comment_elements = driver.find_elements(
                        By.CSS_SELECTOR,
                        self.selectors['comments']['comment_list'])
                    comment_element = comment_elements[i]

                    # data-info ì†ì„±ì—ì„œ ëŒ“ê¸€ ì •ë³´ ì¶”ì¶œ
                    data_info = comment_element.get_attribute('data-info')
                    if not data_info:
                        continue


                    # ì‚­ì œëœ ëŒ“ê¸€ì¸ì§€ í™•ì¸
                    is_deleted = 'deleted:true' in data_info

                    if is_deleted:
                        # ì‚­ì œëœ ëŒ“ê¸€ì˜ ê²½ìš° ì œí•œëœ ì •ë³´ë§Œ ì¶”ì¶œ
                        content = "ì‚­ì œëœ ëŒ“ê¸€ì…ë‹ˆë‹¤"
                        author = self._extract_text_by_selector(
                            comment_element, self.selectors['comments']['deleted_comment_author'])
                        like_count = ""
                        dislike_count = ""
                    else:
                        # ì¼ë°˜ ëŒ“ê¸€ ì •ë³´ ì¶”ì¶œ
                        content = self._extract_text_by_selector(
                            comment_element, self.selectors['comments']['comment_content'])
                        author = self._extract_text_by_selector(
                            comment_element, self.selectors['comments']['comment_author'])
                        like_count = self._extract_number_from_text(
                            self._extract_text_by_selector(
                                comment_element, self.selectors['comments']['comment_like'])
                        )
                        dislike_count = self._extract_number_from_text(
                            self._extract_text_by_selector(
                                comment_element, self.selectors['comments']['comment_dislike'])
                        )


                    # ì‘ì„± ë‚ ì§œ ì¶”ì¶œ
                    created_at = self._extract_text_by_selector(
                        comment_element, self.selectors['comments']['comment_date'])

                    # ëŒ“ê¸€ ë°ì´í„° ìƒì„±
                    comment_data = {
                        'article_id': article_id,
                        'comment_id': self.comment_id_counter,
                        'content': content,
                        'author': author,
                        'like_count': like_count,
                        'dislike_count': dislike_count,
                        'created_at': created_at,
                        'scraped_at': scraped_at
                    }

                    self.comments_data.append(comment_data)
                    self.comment_id_counter += 1

                except Exception as e:
                    self.logger.debug(f"ê°œë³„ ëŒ“ê¸€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue

            self.logger.info(f"  â”œâ”€ ëŒ“ê¸€ ì¶”ì¶œ ì™„ë£Œ: {len(comment_elements)}ê°œ")

        except Exception as e:
            self.logger.error(f"ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
    

    def _disable_cleanbot(self, driver: webdriver.Chrome) -> None:
      try:
          # í´ë¦°ë´‡ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
          cleanbot_container = driver.find_element(
              By.CSS_SELECTOR, self.selectors['cleanbot']["cleanbot_container"])
          
          # í´ë¦°ë´‡ í•´ì œ ì ê²€
          cleanbot_message = self._extract_text_by_selector(
              cleanbot_container, self.selectors['cleanbot']["cleanbot_message"])
          
          if cleanbot_message and "ì°©í•œëŒ“ê¸€" in cleanbot_message:
              self.logger.info("  â”œâ”€ í´ë¦°ë´‡ í•´ì œ í™•ì¸")
              return
            
          setting_button = cleanbot_container.find_element(
              By.CSS_SELECTOR, self.selectors['cleanbot']["setting_button"])

          if setting_button.is_displayed():
              setting_button.click()
              print("    âœ“ ì„¤ì • ë²„íŠ¼ í´ë¦­ - ëª¨ë‹¬ì°½ ëŒ€ê¸°ì¤‘...")
              time.sleep(3)  # ëª¨ë‹¬ ìƒì„± ëŒ€ê¸° ì‹œê°„ ì¦ê°€

              # ì •í™•í•œ ëª¨ë‹¬ ì„ íƒìë“¤ (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)
              modal_selectors = {
                  "cleanbot_modal": ".u_cbox_layer_wrap, .u_cbox_layer_cleanbot2_wrap, .u_cbox_layer_cleanbot2, .u_cbox_layer_cleanbot2_content"
              }

              # ì²´í¬ë°•ìŠ¤ ì„ íƒìë“¤ (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)
              checkbox_selectors = {
                  "cleanbot_checkbox": "#cleanbot_dialog_checkbox_cbox_module, .u_cbox_layer_cleanbot2_checkbox, input[data-action='toggleCleanbot2']"
              }

              # í™•ì¸ ë²„íŠ¼ ì„ íƒìë“¤ (ì‹¤ì œ HTML êµ¬ì¡° ê¸°ë°˜)
              confirm_selectors = {
                  "confirm_button": "button[data-action='updateCleanbotStatus'], .u_cbox_layer_cleanbot2_extrabtn"
              }

              # ëª¨ë‹¬ì°½ ì°¾ê¸°
              modal = None
              for key, selector_string in modal_selectors.items():
                  selectors = [s.strip() for s in selector_string.split(',')]
                  for selector in selectors:
                      try:
                          modal = driver.find_element(By.CSS_SELECTOR, selector)
                          if modal and modal.is_displayed():
                              print(f"    âœ“ ëª¨ë‹¬ì°½ ë°œê²¬: {key} -> {selector}")
                              break
                      except:
                          continue
                  if modal and modal.is_displayed():
                      break

              if not modal or not modal.is_displayed():
                  self.logger.warning("CleanBot ì„¤ì • ëª¨ë‹¬ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                  return

              # ì²´í¬ë°•ìŠ¤ ì°¾ê¸° ë° ìƒíƒœ í™•ì¸
              checkbox = None
              for key, selector_string in checkbox_selectors.items():
                  selectors = [s.strip() for s in selector_string.split(',')]
                  for selector in selectors:
                      try:
                          checkbox = modal.find_element(
                              By.CSS_SELECTOR, selector)
                          if checkbox:
                              print(f"    âœ“ ì²´í¬ë°•ìŠ¤ ë°œê²¬: {key} -> {selector}")
                              break
                      except:
                          continue
                  if checkbox:
                      break

              if not checkbox:
                  self.logger.warning("CleanBot ì²´í¬ë°•ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                  return

              # ì²´í¬ë°•ìŠ¤ ìƒíƒœ í™•ì¸ (is_checked í´ë˜ìŠ¤ ì—¬ë¶€)
              checkbox_classes = checkbox.get_attribute('class') or ""
              is_checked = "is_checked" in checkbox_classes
              print(f"    âœ“ í˜„ì¬ í´ë¦°ë´‡ ìƒíƒœ: {'í™œì„±í™”' if is_checked else 'ë¹„í™œì„±í™”'}")

              # í´ë¦°ë´‡ì´ í™œì„±í™”ë˜ì–´ ìˆìœ¼ë©´ ë¹„í™œì„±í™”
              if is_checked:
                  try:
                      # ì²´í¬ë°•ìŠ¤ í´ë¦­ ì‹œë„
                      driver.execute_script("arguments[0].click();", checkbox)
                      print("    âœ“ í´ë¦°ë´‡ ì²´í¬ë°•ìŠ¤ í´ë¦­ë¨")
                      time.sleep(1)
                  except Exception as e:
                      print(f"    ! ì²´í¬ë°•ìŠ¤ ì§ì ‘ í´ë¦­ ì‹¤íŒ¨: {e}")
                      try:
                          # ë”ë¯¸ ì²´í¬ë°•ìŠ¤ í´ë¦­ ì‹œë„
                          dummy_checkbox = modal.find_element(
                              By.CSS_SELECTOR, ".u_cbox_layer_cleanbot2_checkboxdummy")
                          driver.execute_script(
                              "arguments[0].click();", dummy_checkbox)
                          print("    âœ“ ë”ë¯¸ ì²´í¬ë°•ìŠ¤ í´ë¦­ë¨")
                          time.sleep(1)
                      except Exception as e2:
                          print(f"    ! ë”ë¯¸ ì²´í¬ë°•ìŠ¤ í´ë¦­ë„ ì‹¤íŒ¨: {e2}")
                          try:
                              # ë ˆì´ë¸” í´ë¦­ ì‹œë„
                              label = modal.find_element(
                                  By.CSS_SELECTOR, "label[for='cleanbot_dialog_checkbox_cbox_module']")
                              driver.execute_script(
                                  "arguments[0].click();", label)
                              print("    âœ“ ë ˆì´ë¸” í´ë¦­ë¨")
                              time.sleep(1)
                          except Exception as e3:
                              self.logger.warning(f"ëª¨ë“  ì²´í¬ë°•ìŠ¤ í´ë¦­ ë°©ë²• ì‹¤íŒ¨: {e3}")
                              return

                  # ìƒíƒœ ë³€ê²½ í™•ì¸
                  updated_checkbox = modal.find_element(
                      By.CSS_SELECTOR, "#cleanbot_dialog_checkbox_cbox_module")
                  updated_classes = updated_checkbox.get_attribute('class') or ""
                  is_still_checked = "is_checked" in updated_classes

                  if not is_still_checked:
                      print("    âœ“ í´ë¦°ë´‡ì´ ì„±ê³µì ìœ¼ë¡œ ë¹„í™œì„±í™”ë¨")

                      # í™•ì¸ ë²„íŠ¼ í´ë¦­
                      confirm_button = None
                      for key, selector_string in confirm_selectors.items():
                          selectors = [s.strip()
                                      for s in selector_string.split(',')]
                          for selector in selectors:
                              try:
                                  confirm_button = modal.find_element(
                                      By.CSS_SELECTOR, selector)
                                  if confirm_button and confirm_button.is_displayed():
                                      print(f"    âœ“ í™•ì¸ ë²„íŠ¼ ë°œê²¬: {key} -> {selector}")
                                      break
                              except:
                                  continue
                          if confirm_button and confirm_button.is_displayed():
                              break

                      if confirm_button:
                          driver.execute_script(
                              "arguments[0].click();", confirm_button)
                          print("    âœ“ í™•ì¸ ë²„íŠ¼ í´ë¦­ - ì„¤ì • ì €ì¥ë¨")
                          time.sleep(1)
                          self.logger.info("  â”œâ”€ CleanBot ë¹„í™œì„±í™” ì™„ë£Œ")
                      else:
                          self.logger.warning("í™•ì¸ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                  else:
                      self.logger.warning("í´ë¦°ë´‡ ë¹„í™œì„±í™”ì— ì‹¤íŒ¨í•¨")
              else:
                  self.logger.debug("í´ë¦°ë´‡ì´ ì´ë¯¸ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŒ")
                  # ì´ë¯¸ ë¹„í™œì„±í™”ëœ ê²½ìš°ì—ë„ í™•ì¸ ë²„íŠ¼ í´ë¦­í•´ì„œ ëª¨ë‹¬ ë‹«ê¸°
                  confirm_button = modal.find_element(
                      By.CSS_SELECTOR, "button[data-action='updateCleanbotStatus']")
                  if confirm_button:
                      driver.execute_script(
                          "arguments[0].click();", confirm_button)
                      print("    âœ“ ëª¨ë‹¬ì°½ ë‹«ê¸°")

          else:
              self.logger.warning("CleanBot ì„¤ì • ë²„íŠ¼ì´ ë³´ì´ì§€ ì•ŠìŒ")
              return

      except Exception as e:
          self.logger.warning(f"CleanBot ë°©ì§€ ê¸°ëŠ¥ ë¹„í™œì„±í™” ì‹¤íŒ¨: {e}")
          # ì—ëŸ¬ ë°œìƒ ì‹œ ëª¨ë‹¬ì´ ì—´ë ¤ìˆë‹¤ë©´ ë‹«ê¸° ì‹œë„
          try:
              close_button = driver.find_element(
                  By.CSS_SELECTOR, "button[data-action='closeCleanbotLayer']")
              if close_button and close_button.is_displayed():
                  driver.execute_script("arguments[0].click();", close_button)
                  self.logger.warning("ì—ëŸ¬ ë°œìƒìœ¼ë¡œ ëª¨ë‹¬ì°½ ê°•ì œ ë‹«ê¸°")
          except:
              pass

    def _process_single_url(self, driver: webdriver.Chrome, url: str) -> bool:
        """
        ë‹¨ì¼ URLì˜ ê¸°ì‚¬ + ëŒ“ê¸€ ì²˜ë¦¬

        Args:
            driver: WebDriver ì¸ìŠ¤í„´ìŠ¤
            url: ì²˜ë¦¬í•  URL

        Returns:
            ì²˜ë¦¬ ì„±ê³µ ì—¬ë¶€
        """
        try:
            self.logger.info(f"ì²˜ë¦¬ ì‹œì‘: {url}")

            # 1. ê¸°ì‚¬ ë°ì´í„° ì¶”ì¶œ
            self.logger.info(f"  â”œâ”€ ê¸°ì‚¬ë°ì´í„° ì¶”ì¶œì‹œì‘")
            article_data = self._extract_article_data(driver, url)
            if not article_data:
                return False

            current_article_id = article_data['article_id']

            # 1.1 ëŒ“ê¸€ ì¶”ê°€ì‘ì—… ì§„í–‰ì—¬ë¶€ íŒë‹¨ ë° ì§„í–‰
            sohuld_process_additional_comments_work = article_data['comment_count'] != "0"
            if not sohuld_process_additional_comments_work:
                self.logger.info(
                    f"  â”œâ”€ ëŒ“ê¸€ì´ ì—†ëŠ” ê¸°ì‚¬ ë°ì´í„°ë§Œ ì €ì¥")
                self.logger.info(f"  â””â”€ ì²˜ë¦¬ ì™„ë£Œ {'â”€' * 60}")
                # ëŒ“ê¸€ì´ ì—†ëŠ” ê²½ìš°, ê¸°ì‚¬ ë°ì´í„°ë§Œ ì €ì¥í•˜ê³  ì¢…ë£Œ
                self.articles_data.append(article_data)
                self.article_id_counter += 1
                return True


            # 2. ëŒ“ê¸€ í†µê³„ ì¶”ì¶œ ë° ê¸°ì‚¬ ë°ì´í„° ì—…ë°ì´íŠ¸
            self.logger.info(f"  â”œâ”€ ëŒ“ê¸€ ì¼ë°˜í†µê³„ ì¶”ì¶œì‹œì‘")
            self._extract_comment_stats(driver, article_data)

            # 3. ëŒ“ê¸€ ìƒì„¸ í†µê³„ ì¶”ì¶œ
            self.logger.info(f"  â”œâ”€ ëŒ“ê¸€ ìƒì„¸í†µê³„ ì¶”ì¶œì‹œì‘")
            self._extract_comment_demographic_stats(driver, article_data)

            # 4. ëŒ“ê¸€ í˜ì´ì§€ë¡œ ì´ë™
            self.logger.info(f"  â”œâ”€ ëŒ“ê¸€ í˜ì´ì§€ë¡œ ì´ë™")
            if self._navigate_to_comments_page(driver):
                
                # 5. í´ë¦°ë´‡ í•´ì œ
                self.logger.info(f"  â”œâ”€ í´ë¦°ë´‡ í•´ì œ ì‹œì‘")
                self._disable_cleanbot(driver)
                
                # 6. ëª¨ë“  ëŒ“ê¸€ ë¡œë“œ
                self.logger.info(f"  â”œâ”€ ëŒ“ê¸€ ë¡œë“œ ì‹œì‘")
                self._load_all_comments(driver)

                # 7. ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ
                self.logger.info(f"  â”œâ”€ ëŒ“ê¸€ ì¶”ì¶œ ì‹œì‘")
                self._extract_comments_data(driver, current_article_id)
            else:
                self.logger.warning(f"  â”œâ”€ ëŒ“ê¸€ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨, ê¸°ì‚¬ ë°ì´í„°ë§Œ ì €ì¥: {url}")

            # 8. ê¸°ì‚¬ ë°ì´í„° ì €ì¥
            self.logger.info(f"  â”œâ”€ ê¸°ì‚¬ ë°ì´í„° ì €ì¥(ë©”ëª¨ë¦¬)")
            self.articles_data.append(article_data)
            self.article_id_counter += 1

            self.logger.info(f"  â””â”€ ì²˜ë¦¬ ì™„ë£Œ {'â”€' * 60}")
            return True

        except Exception as e:
            self.logger.error(f"URL ì²˜ë¦¬ ì‹¤íŒ¨ ({url}): {e}")
            return False

    def scrape_urls(self, urls: List[str]) -> None:
        """
        URL ëª©ë¡ í¬ë¡¤ë§ (ìˆœì°¨ ì²˜ë¦¬)

        Args:
            urls: í¬ë¡¤ë§í•  URL ë¦¬ìŠ¤íŠ¸
        """
        delay = self.config['scraping'].get('delay_between_requests', 3.0)

        self.logger.info(
            f"í†µí•© í¬ë¡¤ë§ ì‹œì‘: {len(urls)}ê°œ URL (ìˆœì°¨ ì²˜ë¦¬, ìš”ì²­ ê°„ê²©: {delay}ì´ˆ)")

        # ë‹¨ì¼ WebDriver ìƒì„±
        driver = None
        try:
            driver = self._create_driver()
            self.logger.info("Chrome ë¸Œë¼ìš°ì € ì‹œì‘")

            # ì§„í–‰ë¥  í‘œì‹œë¥¼ ìœ„í•œ tqdm ì„¤ì •
            for url in tqdm(urls, desc="í†µí•© í¬ë¡¤ë§ ì§„í–‰", unit="URL"):
                try:
                    success = self._process_single_url(driver, url)
                    if not success:
                        self.failed_urls.append(url)

                except Exception as e:
                    self.logger.error(f"URL ì²˜ë¦¬ ì˜¤ë¥˜ ({url}): {e}")
                    self.failed_urls.append(url)

                # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—°
                if delay > 0:
                    time.sleep(delay)

        except Exception as e:
            self.logger.error(f"WebDriver ìƒì„±/ì‚¬ìš© ì˜¤ë¥˜: {e}")
            raise
        finally:
            if driver:
                try:
                    driver.quit()
                    self.logger.info("Chrome ë¸Œë¼ìš°ì € ì¢…ë£Œ")
                except:
                    pass

        self.logger.info(
            f"í†µí•© í¬ë¡¤ë§ ì™„ë£Œ: ì„±ê³µ {len(self.articles_data)}ê°œ, ì‹¤íŒ¨ {len(self.failed_urls)}ê°œ")

    def save_csv_files(self, output_dir: str, suffix: str) -> None:
        """
        articles.csvì™€ comments.csv íŒŒì¼ ì €ì¥

        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            suffix: íŒŒì¼ ì ‘ë¯¸ì‚¬ (ì…ë ¥ë°›ì€ urls íŒŒì¼ëª…)
        """
        try:
            # articles.csv ì €ì¥
            if self.articles_data:
                # suffixì—ì„œ íŒŒì¼ëª… ì¶”ì¶œ
                clean_suffix = Path(suffix).stem
                # suffixê¸°ë°˜ íŒŒì¼ëª… ìƒì„±
                articles_file = Path(output_dir) / f"articles_{clean_suffix}.csv"
                with open(articles_file, 'w', newline='', encoding='utf-8') as f:
                    fieldnames = [
                        'article_id', 'url', 'title', 'content', 'author', 'publish_date', 'category',
                        'like_count', 'comment_count', 'active_comment_count', 'deleted_comment_count',
                        'removed_comment_count', 'male_ratio', 'female_ratio', 'age_10s_ratio',
                        'age_20s_ratio', 'age_30s_ratio', 'age_40s_ratio', 'age_50s_ratio',
                        'age_60plus_ratio', 'scraped_at'
                    ]
                    writer = csv.DictWriter(
                        f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)
                    writer.writeheader()
                    writer.writerows(self.articles_data)

                self.logger.info(
                    f"articles.csv ì €ì¥ ì™„ë£Œ: {len(self.articles_data)}ê°œ ê¸°ì‚¬")

            # comments.csv ì €ì¥
            if self.comments_data:
                comments_file = Path(output_dir) / f"comments_{clean_suffix}.csv"
                with open(comments_file, 'w', newline='', encoding='utf-8') as f:
                    fieldnames = [
                        'article_id', 'comment_id', 'parent_comment_id', 'comment_type',
                        'content', 'author', 'like_count', 'dislike_count', 'reply_count',
                        'created_at', 'scraped_at'
                    ]
                    writer = csv.DictWriter(
                        f, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)
                    writer.writeheader()
                    writer.writerows(self.comments_data)

                self.logger.info(
                    f"comments.csv ì €ì¥ ì™„ë£Œ: {len(self.comments_data)}ê°œ ëŒ“ê¸€")

        except Exception as e:
            self.logger.error(f"CSV íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ë„¤ì´ë²„ ë‰´ìŠ¤ í†µí•© í¬ë¡¤ëŸ¬ (ê¸°ì‚¬ + ëŒ“ê¸€)')
    parser.add_argument('--urls', required=True, help='URL ëª©ë¡ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--config', default='config.json', help='ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output', default='output/', help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--upload', action='store_true', help='êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì‘ì—…ë‚´ìš© ì—…ë¡œë“œ')

    args = parser.parse_args()

    try:
        # ì„¤ì • ë¡œë“œ ë° ê²€ì¦
        config = load_config(args.config)
        if not validate_config(config):
            sys.exit(1)

        # ë¡œê¹… ì„¤ì •
        logger = setup_logger(config)

        # ì‹œì‘ ë¡œê·¸
        logger.info("=" * 50)
        logger.info("ğŸš€ ë„¤ì´ë²„ ë‰´ìŠ¤ í†µí•© í¬ë¡¤ëŸ¬ ì‹œì‘")
        logger.info("=" * 50)

        # ì‹œìŠ¤í…œ ì •ë³´ ë¡œê·¸
        system_info = get_system_info()
        logger.info(f"ì‹œìŠ¤í…œ ì •ë³´: {system_info}")

        # URL ë¡œë“œ
        urls = load_urls(args.urls)
        if not urls:
            logger.error("í¬ë¡¤ë§í•  URLì´ ì—†ìŠµë‹ˆë‹¤")
            sys.exit(1)

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = create_output_directory(args.output)

        # í†µí•© í¬ë¡¤ëŸ¬ ì‹¤í–‰
        scraper = NaverNewsMainScraper(config)
        start_time = datetime.now()

        scraper.scrape_urls(urls)

        end_time = datetime.now()
        duration = end_time - start_time

        # CSV íŒŒì¼ ì €ì¥
        scraper.save_csv_files(output_dir, args.urls)

        # ì‹¤íŒ¨í•œ URL ì €ì¥
        if scraper.failed_urls:
            from utils import save_failed_urls
            save_failed_urls(scraper.failed_urls, output_dir)

        # êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—…ë¡œë“œ
        if args.upload:
            logger.info("êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì—…ë¡œë“œ ì‹œì‘")
            
            # suffixê¸°ë°˜ íŒŒì¼ëª… ìƒì„±
            clean_suffix = Path(args.urls).stem
            articles_file = Path(output_dir) / f"articles_{clean_suffix}.csv"
            comments_file = Path(output_dir) / f"comments_{clean_suffix}.csv"
            try:
                uploader = DriveUploader('auth/credentials.json')
                uploader.upload_file(articles_file)
                logger.info(f"êµ¬ê¸€ ë“œë¼ì´ë¸Œì— articles ì—…ë¡œë“œ ì™„ë£Œ: {articles_file}")
                uploader.upload_file(comments_file)
                logger.info(f"êµ¬ê¸€ ë“œë¼ì´ë¸Œì— articles ì—…ë¡œë“œ ì™„ë£Œ: {comments_file}")
            except Exception as e:
                logger.warning(f"êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                raise
            
            
        # ìµœì¢… ê²°ê³¼ ë¦¬í¬íŠ¸
        logger.info(f"í†µí•© í¬ë¡¤ë§ ì™„ë£Œ!")
        logger.info(f"ì†Œìš” ì‹œê°„: {duration}")
        logger.info(f"ì„±ê³µí•œ ê¸°ì‚¬: {len(scraper.articles_data)}ê°œ")
        logger.info(f"ìˆ˜ì§‘í•œ ëŒ“ê¸€: {len(scraper.comments_data)}ê°œ")
        logger.info(f"ì‹¤íŒ¨í•œ URL: {len(scraper.failed_urls)}ê°œ")

        if scraper.articles_data:
            success_rate = len(scraper.articles_data) / len(urls) * 100
            logger.info(f"ì„±ê³µë¥ : {success_rate:.1f}%")

            # ëŒ“ê¸€ í†µê³„
            avg_comments = len(scraper.comments_data) / \
                len(scraper.articles_data)
            logger.info(f"í‰ê·  ëŒ“ê¸€ ìˆ˜: {avg_comments:.1f}ê°œ/ê¸°ì‚¬")

    except KeyboardInterrupt:
        print("\ní¬ë¡¤ë§ì´ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(0)
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
